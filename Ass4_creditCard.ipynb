{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e_xTtLpozD2"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "# pandas: For data manipulation and analysis\n",
    "# numpy: For numerical computations\n",
    "# tensorflow: For building and training the autoencoder model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DIqFiMGpKaC"
   },
   "outputs": [],
   "source": [
    "# Read the credit card transaction dataset from CSV file\n",
    "# The dataset contains various features of credit card transactions\n",
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "TmcH2BDgpQ22",
    "outputId": "a7296576-986b-4ccd-8acb-bae67c059e7c"
   },
   "outputs": [],
   "source": [
    "# Display the contents of the DataFrame\n",
    "# This helps us inspect the data and understand its structure\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4S19NFapRRX",
    "outputId": "fe144958-812f-4ec0-e31d-3c8571215f04"
   },
   "outputs": [],
   "source": [
    "# Check the dimensions of the dataset\n",
    "# Returns a tuple (number of rows, number of columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F2h3TYdpSqN"
   },
   "outputs": [],
   "source": [
    "# Remove 'Time' and 'Class' columns from the dataset\n",
    "# Time: Not relevant for anomaly detection\n",
    "# Class: We don't use labels in unsupervised anomaly detection\n",
    "df = df.drop(['Time','Class'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "8p0vP8qBpjyC",
    "outputId": "fcf3b220-0c4e-4f01-a648-0933f16a8078"
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame after removing Time and Class columns\n",
    "# Verify that the columns were successfully removed\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uRA1cFGplL6"
   },
   "outputs": [],
   "source": [
    "# Import required preprocessing tools from scikit-learn\n",
    "# StandardScaler: For standardizing the features by removing the mean and scaling to unit variance\n",
    "# train_test_split: For splitting the dataset into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TblHslaNql9-"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data:\n",
    "# 1. Remove any rows with missing values\n",
    "# 2. Standardize the features (zero mean and unit variance)\n",
    "# 3. Split data into training (80%) and testing (20%) sets\n",
    "df.dropna(inplace=True)\n",
    "scaler = StandardScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "x_train,x_test = train_test_split(df,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_B9gl7dqtgp",
    "outputId": "f1c4cc78-25c0-4697-f324-a9a154b7608e"
   },
   "outputs": [],
   "source": [
    "# Check the shape of training data\n",
    "# This will help verify the split ratio and number of features\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFp28KIiqvzC"
   },
   "outputs": [],
   "source": [
    "# Import necessary layers from Keras for building the autoencoder\n",
    "# Sequential: For creating the model layer by layer\n",
    "# Dense: Fully connected layer\n",
    "# Dropout: For regularization to prevent overfitting\n",
    "# Input: For specifying input shape\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TKDSkfugtb7N",
    "outputId": "1e960dba-5ad6-4ac4-ae6b-e231a6a09639"
   },
   "outputs": [],
   "source": [
    "# Build and train the autoencoder model\n",
    "\n",
    "# Create an autoencoder with symmetric encoder and decoder architecture\n",
    "model = Sequential([\n",
    "    # Encoder part\n",
    "    Input(shape=(x_train.shape[1],)),     # Input layer matching feature dimensions\n",
    "    Dense(32, activation='relu'),          # Compress to 32 dimensions\n",
    "    Dropout(0.2),                         # Prevent overfitting\n",
    "    Dense(16, activation='relu'),          # Further compress to 16 dimensions\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),           # Bottleneck layer - 8 dimensions\n",
    "\n",
    "    # Decoder part - mirror the encoder architecture\n",
    "    Dense(16, activation='relu'),          # Start expanding back\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),          # Continue expanding\n",
    "    Dense(x_train.shape[1], activation='linear')  # Output layer matching input dimensions\n",
    "])\n",
    "\n",
    "# Configure the model for training\n",
    "model.compile(optimizer='adam',            # Adam optimizer for efficient training\n",
    "             loss='mean_squared_error')    # MSE loss for reconstruction error\n",
    "\n",
    "# Train the autoencoder\n",
    "# Note: Input = Output in autoencoders\n",
    "history = model.fit(x_train,\n",
    "                   x_train,                # Same data for input and target\n",
    "                   epochs=20,              # Number of training iterations\n",
    "                   validation_data=(x_test, x_test),  # Validation data\n",
    "                   batch_size=30,          # Number of samples per gradient update\n",
    "                   shuffle=True)           # Shuffle data each epoch\n",
    "\n",
    "# Plot training history\n",
    "import seaborn as sns\n",
    "sns.lineplot(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIha_cmTya5o",
    "outputId": "d91a15b9-7037-468c-b1ec-e8e69a9a2eb7"
   },
   "outputs": [],
   "source": [
    "# Calculate reconstruction error for anomaly detection\n",
    "# 1. Get model predictions (reconstructions) for test data\n",
    "# 2. Calculate Mean Squared Error between original and reconstructed data\n",
    "# Higher MSE indicates potential anomalies\n",
    "predictions = model.predict(x_test)\n",
    "mse = np.mean(np.power(x_test - predictions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMv6BGzh1Rno",
    "outputId": "1ae32911-50c5-49ec-8cc4-2a3193b15c71"
   },
   "outputs": [],
   "source": [
    "# Set anomaly threshold at 95th percentile of MSE values\n",
    "# This means we consider the top 5% of reconstruction errors as anomalies\n",
    "# Adjust percentile based on expected anomaly rate in your data\n",
    "threshold = np.percentile(mse, 95)  # Using 95th percentile\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyT7JUXU4SFh"
   },
   "outputs": [],
   "source": [
    "# Create boolean mask for anomalies\n",
    "# True where MSE > threshold (anomalies)\n",
    "# False where MSE <= threshold (normal transactions)\n",
    "anomalies = mse > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHl7fbeU4b1E",
    "outputId": "c554f3c7-5d10-4b07-c09f-d2ea8d65ee8b"
   },
   "outputs": [],
   "source": [
    "# Count and display the total number of detected anomalies\n",
    "# This helps understand how many transactions were flagged as fraudulent\n",
    "num_anomalies = np.sum(anomalies)\n",
    "print(f\"Number of Anomalies: {num_anomalies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "oNGYBSjy4eQ7",
    "outputId": "2bce110e-4bf9-4bd8-ed15-582e16cd25f3"
   },
   "outputs": [],
   "source": [
    "# Visualize the distribution of reconstruction errors (MSE)\n",
    "# - Blue dots: MSE for each sample\n",
    "# - Red line: Anomaly threshold\n",
    "# Points above the red line are considered anomalies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mse, marker='o', linestyle='', markersize=3, label='MSE')\n",
    "plt.axhline(threshold, color='r', linestyle='--', label='Anomaly Threshold')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Anomaly Detection Results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "SJGopnqz4i9j",
    "outputId": "b9f76c38-0d72-4239-e083-8ce45b7f5bc6"
   },
   "outputs": [],
   "source": [
    "# Plot comparison of original vs reconstructed data for a normal transaction\n",
    "# This visualization helps understand how well the autoencoder reconstructs normal patterns\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_test[0], label='Original Transaction')\n",
    "plt.plot(predictions[0], label='Reconstructed Transaction')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend()\n",
    "plt.title('Normal Transaction: Original vs Reconstructed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "oBiJwgy74ti-",
    "outputId": "c4678063-869d-48f2-d6ef-37ad32a3c42c"
   },
   "outputs": [],
   "source": [
    "# Create and display confusion matrix for anomaly detection\n",
    "# Note: In this case, we're comparing predictions against themselves\n",
    "# This is just to visualize the distribution of anomalies\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "sns.heatmap(confusion_matrix(anomalies, anomalies), annot = True, fmt = 'd')\n",
    "plt.xlabel(\"Predicted label\", fontsize = 14)\n",
    "plt.ylabel(\"True label\", fontsize = 14)\n",
    "plt.title(\"Confusion Matrix\", fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJIOp_us6KLL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
