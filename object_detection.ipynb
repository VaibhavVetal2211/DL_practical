{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd8f99-cadc-408f-af7a-b26861159e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries for Object Detection\n",
    "# Purpose: Set up necessary Python libraries for transfer learning with VGG16\n",
    "\n",
    "# os: Operating system interface\n",
    "# - Used for file and path operations\n",
    "\n",
    "# matplotlib.pyplot: Visualization library\n",
    "# - For plotting training progress and results\n",
    "\n",
    "# tensorflow: Deep learning framework\n",
    "# - Core library for neural networks\n",
    "# - Provides high-level Keras API\n",
    "\n",
    "# Key Keras components:\n",
    "# - layers: Building blocks for neural networks\n",
    "# - models: Ways to organize layers\n",
    "# - optimizers: Algorithms to train networks\n",
    "# - VGG16: Pre-trained CNN for image recognition\n",
    "# - Flatten: Converts 2D features to 1D\n",
    "# - Dense: Fully connected layer\n",
    "# - Dropout: Regularization layer to prevent overfitting\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb7905-a6ef-4754-9a4f-940fb0800929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Settings\n",
    "# Purpose: Define key parameters for model training\n",
    "\n",
    "# DATASET_DIR: Path to Caltech-101 dataset\n",
    "# - Contains images organized in class folders\n",
    "\n",
    "# VGG_WEIGHTS_PATH: Path to pre-trained VGG16 weights\n",
    "# - 'notop' indicates weights without classification layers\n",
    "# - Used for transfer learning\n",
    "\n",
    "# IMG_SIZE: Input image dimensions (224, 224)\n",
    "# - Standard size for VGG16\n",
    "# - All images will be resized to this\n",
    "\n",
    "# BATCH_SIZE: Number of images processed together\n",
    "# - 32 is a common choice balancing memory and speed\n",
    "# - Affects training stability and speed\n",
    "\n",
    "# SEED: Random seed for reproducibility\n",
    "# - Ensures consistent train/validation splits\n",
    "# - Makes results reproducible\n",
    "\n",
    "DATASET_DIR = \"Downloads/LP-IV-datasets-20251107T052642Z-1-001/LP-IV-datasets/Object Detection(Ass6)/caltech-101-img\"\n",
    "VGG_WEIGHTS_PATH = \"Downloads/LP-IV-datasets-20251107T052642Z-1-001/LP-IV-datasets/Object Detection(Ass6)/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "SEED = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0cfec-1bbc-4a00-b71b-1985be111b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "# Purpose: Load and prepare image data for training\n",
    "\n",
    "# Create training dataset:\n",
    "# Parameters:\n",
    "# - validation_split=0.2: 20% of data for validation\n",
    "# - subset=\"training\": Get training portion of split\n",
    "# - seed=SEED: For reproducible splitting\n",
    "# - image_size=IMG_SIZE: Resize all images\n",
    "# - batch_size=BATCH_SIZE: Images per batch\n",
    "# - label_mode='categorical': One-hot encoded labels\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'   # For categorical_crossentropy loss\n",
    ")\n",
    "\n",
    "# Create validation dataset with same parameters\n",
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get class information\n",
    "class_names = train_data.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"Found {num_classes} classes. Example: {class_names[:8]}\")\n",
    "\n",
    "# Optimize data pipeline:\n",
    "# cache(): Keeps data in memory after first epoch\n",
    "# prefetch(): Prepares next batch while current processes\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145d533-373f-40ad-a482-72d4e16814c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Configure VGG16 Base Model\n",
    "# Purpose: Set up pre-trained VGG16 for transfer learning\n",
    "\n",
    "# Load VGG16 model:\n",
    "# Parameters:\n",
    "# - weights=VGG_WEIGHTS_PATH: Use pre-trained weights\n",
    "# - include_top=False: Exclude classification layers\n",
    "# - input_shape=(*IMG_SIZE, 3): Expected image dimensions\n",
    "base_model = VGG16(weights=VGG_WEIGHTS_PATH, \n",
    "                  include_top=False, \n",
    "                  input_shape=(*IMG_SIZE, 3))\n",
    "\n",
    "# Freeze base model layers:\n",
    "# - Prevents updating VGG16 weights during training\n",
    "# - Preserves learned feature extractors\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea2349-3cab-4c53-8f31-4525b1646d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Custom Classifier\n",
    "# Purpose: Add classification layers on top of VGG16\n",
    "\n",
    "model = models.Sequential([\n",
    "    # Base VGG16 model (frozen)\n",
    "    base_model,\n",
    "    \n",
    "    # Flatten: Convert 3D feature maps to 1D vector\n",
    "    Flatten(),\n",
    "    \n",
    "    # First Dense Layer:\n",
    "    # - 512 neurons with ReLU activation\n",
    "    # - Learns high-level features\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    # First Dropout Layer:\n",
    "    # - Randomly drops 50% of inputs\n",
    "    # - Prevents overfitting\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Second Dense Layer:\n",
    "    # - 256 neurons with ReLU activation\n",
    "    # - Further feature processing\n",
    "    Dense(256, activation='relu'),\n",
    "    \n",
    "    # Second Dropout Layer:\n",
    "    # - Another 50% dropout for regularization\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output Layer:\n",
    "    # - num_classes neurons (one per class)\n",
    "    # - Softmax activation for probability distribution\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbdc0fc-902f-4e5a-9032-542f6f6fb87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compilation\n",
    "# Purpose: Configure training parameters\n",
    "\n",
    "# Parameters:\n",
    "# optimizer='adam': \n",
    "# - Adaptive learning rate optimization\n",
    "# - Good default choice for many problems\n",
    "\n",
    "# loss='categorical_crossentropy':\n",
    "# - Appropriate for multi-class classification\n",
    "# - Works with one-hot encoded labels\n",
    "\n",
    "# metrics=['accuracy']:\n",
    "# - Track classification accuracy during training\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d5799-0219-443b-8b16-89d9fd15a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model Training\n",
    "# Purpose: Train the model with frozen VGG16 layers\n",
    "\n",
    "# Parameters:\n",
    "# train_data: Training dataset\n",
    "# validation_data: For monitoring performance\n",
    "# epochs=8: Number of complete passes through data\n",
    "# \n",
    "# Returns:\n",
    "# history: Contains training metrics per epoch\n",
    "# - Useful for plotting learning curves\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c1749-0d41-4afb-a725-8cb65c8fc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Fine-tuning\n",
    "# Purpose: Fine-tune last few VGG16 layers for better performance\n",
    "\n",
    "# Steps (currently commented out):\n",
    "# 1. Unfreeze last 4 layers of VGG16\n",
    "# 2. Recompile with very small learning rate (1e-5)\n",
    "#    - Small rate prevents destroying pre-trained features\n",
    "# 3. Train for additional epochs\n",
    "#\n",
    "# Note: Uncomment to use fine-tuning\n",
    "\n",
    "# for layer in base_model.layers[-4:]:\n",
    "#     layer.trainable = True\n",
    "# model.compile(optimizer=optimizers.Adam(1e-5), \n",
    "#              loss='categorical_crossentropy', \n",
    "#              metrics=['accuracy'])\n",
    "# history_ft = model.fit(train_data, \n",
    "#                       validation_data=val_data, \n",
    "#                       epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf11573-613e-4d35-8b1b-e0d61dab6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# Purpose: Assess model performance on validation set\n",
    "\n",
    "# model.evaluate returns:\n",
    "# - loss: Value of loss function\n",
    "# - acc: Classification accuracy\n",
    "# \n",
    "# Print accuracy as percentage for readability\n",
    "loss, acc = model.evaluate(val_data)\n",
    "print(f\"Validation Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad28d7-c132-4bd9-906d-86e8d726316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Progress\n",
    "# Purpose: Plot accuracy curves for analysis\n",
    "\n",
    "# Plot both training and validation accuracy:\n",
    "# - x-axis: Training epochs\n",
    "# - y-axis: Accuracy values\n",
    "# \n",
    "# Helps identify:\n",
    "# - Learning progress\n",
    "# - Overfitting (if validation accuracy drops)\n",
    "# - Training stability\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
