{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320c002-7491-4832-89ed-bbffcc6b2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries for CBOW (Continuous Bag of Words) Implementation\n",
    "# Purpose: Set up necessary Python libraries for NLP and deep learning\n",
    "\n",
    "# pandas (pd): Data manipulation library\n",
    "# - Used for data handling and preprocessing\n",
    "\n",
    "# numpy (np): Numerical computing library\n",
    "# - Handles array operations and mathematical computations\n",
    "\n",
    "# re: Regular expressions library\n",
    "# - Used for text cleaning and pattern matching\n",
    "\n",
    "# tensorflow (tf): Deep learning framework\n",
    "# - Provides tools for building and training neural networks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b27d2-7d0e-460f-b9ff-fc202c23ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Data Loading and Preprocessing\n",
    "# Purpose: Load and clean text data for CBOW model training\n",
    "\n",
    "# Step 1: Load text file\n",
    "# - Open file in read mode with UTF-8 encoding\n",
    "# - Read entire content into 'text' variable\n",
    "with open('CBOW.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Split text into sentences\n",
    "# - Use period (.) as sentence delimiter\n",
    "sentences = text.split('.')\n",
    "\n",
    "# Step 3: Clean sentences\n",
    "# Parameters for cleaning:\n",
    "# - re.sub('[^A-Za-z0-9]+', ' ', sen): \n",
    "#   * Remove all characters except letters and numbers\n",
    "#   * Replace special chars with space\n",
    "# - lower(): Convert to lowercase\n",
    "# - strip(): Remove leading/trailing whitespace\n",
    "clean_sen = []\n",
    "for sen in sentences:\n",
    "    if sen.strip() == \"\":  # Skip empty sentences\n",
    "        continue\n",
    "    sen = re.sub('[^A-Za-z0-9]+', ' ', sen)\n",
    "    sen = sen.lower().strip()\n",
    "    clean_sen.append(sen)\n",
    "\n",
    "# Display cleaned sentences\n",
    "print(clean_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b67cee-83f0-4b4b-b940-53d1ee13f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tokenizer from Keras\n",
    "# Purpose: Convert text to numerical sequences\n",
    "\n",
    "# Tokenizer: Tool for text vectorization\n",
    "# - Builds vocabulary from text\n",
    "# - Converts words to unique integers\n",
    "# - Handles text-to-sequence conversion\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cac0f4-56bc-49f9-b982-76ee7136cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text Data\n",
    "# Purpose: Convert sentences to sequences of integers\n",
    "\n",
    "# Create and configure tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fit_on_texts: Build vocabulary from text data\n",
    "# - Creates word-to-index mapping\n",
    "# - Updates internal vocabulary statistics\n",
    "tokenizer.fit_on_texts(clean_sen)\n",
    "\n",
    "# texts_to_sequences: Convert text to integer sequences\n",
    "# - Each word is replaced by its corresponding index\n",
    "seq = tokenizer.texts_to_sequences(clean_sen)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ea356-e441-4f60-9a4a-b13da1ee846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training Data for CBOW Model\n",
    "# Purpose: Create context-target pairs for training\n",
    "\n",
    "# Parameters:\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Total unique words + 1 for padding\n",
    "emb_size = 10     # Dimensionality of word embeddings\n",
    "context_size = 2  # Number of words before/after target to use as context\n",
    "\n",
    "# Create training pairs:\n",
    "# - contexts: Lists of words around target word\n",
    "# - targets: Center words to predict\n",
    "contexts = []\n",
    "targets = []\n",
    "\n",
    "# Generate context-target pairs from sequences\n",
    "for sequence in seq:\n",
    "    # Iterate through each sequence with context window\n",
    "    for i in range(context_size, len(sequence)-context_size):\n",
    "        target = sequence[i]  # Center word\n",
    "        # Context: 2 words before and 2 words after target\n",
    "        context = [sequence[i-2], sequence[i-1], sequence[i+1], sequence[i+2]]\n",
    "        targets.append(target)\n",
    "        contexts.append(context)\n",
    "\n",
    "print(contexts)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60fe82d-17dc-4832-a0e9-47301520bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Training Data to NumPy Arrays\n",
    "# Purpose: Prepare data for model training\n",
    "\n",
    "# X: Context words (input)\n",
    "# - Shape: (n_samples, context_window_size)\n",
    "X = np.array(contexts)\n",
    "\n",
    "# Y: Target words (output)\n",
    "# - Shape: (n_samples,)\n",
    "Y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec904920-c542-4b1a-a850-d0097bdaed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Keras Components\n",
    "# Purpose: Import layers for building CBOW model\n",
    "\n",
    "# Sequential: Linear stack of layers\n",
    "# Dense: Fully connected neural network layer\n",
    "# Embedding: Special layer for word embeddings\n",
    "# Lambda: Custom layer for arbitrary operations\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162f82f-66e1-4841-b38d-f53c0cbcbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Train CBOW Model\n",
    "# Purpose: Create and train the CBOW neural network\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding Layer:\n",
    "    # - input_dim: Size of vocabulary\n",
    "    # - output_dim: Size of embedding vector\n",
    "    # - input_length: Length of input sequences (context window * 2)\n",
    "    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size),\n",
    "    \n",
    "    # Lambda Layer:\n",
    "    # - Averages context word embeddings\n",
    "    # - tf.reduce_mean averages along axis 1 (sequence dimension)\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
    "    \n",
    "    # Hidden Layers:\n",
    "    Dense(256, activation='relu'),  # First hidden layer with 256 neurons\n",
    "    Dense(512, activation='relu'),  # Second hidden layer with 512 neurons\n",
    "    \n",
    "    # Output Layer:\n",
    "    # - vocab_size neurons (one per word)\n",
    "    # - softmax activation for probability distribution\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model:\n",
    "# optimizer='adam': Adaptive learning rate optimization\n",
    "# loss='sparse_categorical_crossentropy': For integer targets\n",
    "# metrics=['accuracy']: Track prediction accuracy\n",
    "model.compile(optimizer='adam', \n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Train model:\n",
    "# X: Context word indices\n",
    "# Y: Target word indices\n",
    "# epochs=80: Number of complete passes through the data\n",
    "history = model.fit(X, Y, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e65e69-5488-49d6-a882-be208f707497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Progress\n",
    "# Purpose: Plot training metrics over epochs\n",
    "\n",
    "# seaborn lineplot:\n",
    "# - x-axis: Epochs\n",
    "# - y-axis: Loss and Accuracy values\n",
    "# - Shows how model performance changes during training\n",
    "import seaborn as sns\n",
    "sns.lineplot(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44e6f4-7ed5-4ce2-8d9b-651e7ad511ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CBOW Model\n",
    "# Purpose: Evaluate model on a test sentence\n",
    "\n",
    "# Step 1: Prepare test sentence\n",
    "test_sentence = [\"an important point difference\"]\n",
    "\n",
    "# Step 2: Convert to sequence\n",
    "# - Use same tokenizer as training\n",
    "# - Convert words to integer indices\n",
    "test_seq = tokenizer.texts_to_sequences(test_sentence)\n",
    "test_seq = test_seq[0]  # Flatten list of lists\n",
    "print(\"Test sequence:\", test_seq)\n",
    "\n",
    "# Step 3: Create context window\n",
    "context_size = 2  # Same as training\n",
    "center_index = len(test_seq) // 2  # Choose middle word as target\n",
    "\n",
    "# Step 4: Extract context words\n",
    "# - Get words before and after target\n",
    "# - Skip target word itself\n",
    "test_context = []\n",
    "for j in range(center_index - context_size, center_index + context_size + 1):\n",
    "    if j == center_index:  # Skip target word\n",
    "        continue\n",
    "    if 0 <= j < len(test_seq):  # Check bounds\n",
    "        test_context.append(test_seq[j])\n",
    "\n",
    "# Step 5: Make prediction\n",
    "x_test = np.array([test_context])\n",
    "print(\"Context indices:\", x_test)\n",
    "\n",
    "# Get prediction and convert back to word\n",
    "pred = model.predict(x_test)\n",
    "pred_index = np.argmax(pred[0])  # Get most likely word index\n",
    "pred_word = index_to_word.get(pred_index, \"<UNK>\")  # Convert to word\n",
    "\n",
    "# Print results\n",
    "print(\"\\nContext words:\", [index_to_word[i] for i in test_context])\n",
    "print(\"Predicted target word =\", pred_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd7779-7e71-4558-8bf7-935049beb124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
